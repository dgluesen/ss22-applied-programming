{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:image width px; font-size:75%; text-align:right;\">\n",
    "    <img src=\"img/djim-loic-ft0-Xu4nTvA-unsplash.jpg\" width=\"width\" height=\"height\" style=\"padding-bottom:0.2em;\" />\n",
    "    <figcaption>Photo by Djim Loic on Unsplash</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "**Applied Programming - Summer term 2022 - FOM Hochschule f√ºr Oekonomie und Management - Cologne**\n",
    "\n",
    "**Lecture 10 - May 31, 2022**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dennis Gluesenkamp*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* [Data analysis lifecycle models](#lifecycle)\n",
    "* [git](#git)\n",
    "* [SQL](#sql)\n",
    "* [Python](#python)\n",
    "* [NumPy](#numpy)\n",
    "* [pandas](#pandas)\n",
    "* [Data preprocessing](#preprocessing)\n",
    "* [Machine Learning](#ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Data analysis lifecycle models<a class=\"anchor\" id=\"lifecycle\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:image width px; font-size:75%; text-align:right;\">\n",
    "    <img src=\"img/crisp.png\" width=\"500\" style=\"padding-bottom:0.2em;\" />\n",
    "    <figcaption>CRISP-DM process diagram (Source: Kenneth Jensen, CC BY-SA 3.0)</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lifecycle models are process descriptions that are helpful or even required when performing data-driven activities. One of these models is CRISP-DM, where the acronym stands for **CR**oss-**I**ndustry **S**tandard **P**rocess for **D**ata **M**ining. It consists of the following steps shown in the diagram:\n",
    "1. Business Understanding:\n",
    "    - Formulation of specific issues and goals\n",
    "    - Alignment of tasks and expectations\n",
    "    - Agreement on a procedure/plan\n",
    "    - Identification of important influencing factors\n",
    "    - Understanding of the business model\n",
    "    - Definition of success criteria\n",
    "2. Data Understanding:\n",
    "    - Consideration of the data inventory\n",
    "    - Evaluation of data availability, reliability, quality\n",
    "    - Identify and describe (statistical) anomalies in the data\n",
    "    - Reconciliation with data protection\n",
    "3. Data Preparation:\n",
    "    - Data cleansing and transformations\n",
    "    - Data linking and aggregation\n",
    "    - Feature Engineering\n",
    "    - Feature Selection\n",
    "4. Modeling:\n",
    "    - Definition of assumptions and framework for modeling\n",
    "    - Selection of suitable algorithms\n",
    "    - Test Design\n",
    "    - Training of the model\n",
    "    - In-depth, targeted data exploration\n",
    "5. Evaluation:\n",
    "    - Comparison of the different models based on quality criteria\n",
    "    - Consideration of the interpretability of the model\n",
    "    - Critical analysis of the modeling process\n",
    "    - Comparison with (economic) success criteria\n",
    "    - Definition of follow-up activities\n",
    "6. Deployment:\n",
    "    - Communication of results\n",
    "    - Integration of the model into the system landscape and decision-making processes\n",
    "    - Maintenance and support of the model\n",
    "    - Documentation of findings and functionality\n",
    "\n",
    "Other lifecycle models include **K**nowledge **D**iscovery in **D**atabases or **S**ample, **E**xplore, **M**odify, **M**odel, and **A**ssess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## git<a class=\"anchor\" id=\"git\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:image width px; font-size:75%; text-align:right;\">\n",
    "    <img src=\"img/git.png\" width=\"800\" style=\"padding-bottom:0.2em;\" />\n",
    "    <figcaption>Selection of important git commands</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The git commands shown in the diagram have the following functions:\n",
    "* ``git status``: Display the files being edited in the working directory, i.e., the current state of the local git directory.\n",
    "* ``git add example.py``: Add the example.py file to the staging area, preparing it for the next commit.\n",
    "* ``git commit -m 'Change A, B, and C'``: Commit the files in the staging area to the repository and provides this commit with the documenting message.\n",
    "* ``git push origin main``: Commit the local version of the files to the remote repository on the named branch, here in the example \"main\".\n",
    "* ``git pull``: Update the local repository with the version/commits of the remote branch/repository.\n",
    "* ``git init``: Create a new local git directory.\n",
    "* ``git clone https://github.com/user/repository.git``: Download an entire remote repository via a URL and set up a local version of it, in the example from GitHub.\n",
    "\n",
    "Please also refer to the PDF files stored in the Online Campus for the git commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## SQL<a class=\"anchor\" id=\"sql\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, only the functionality of SQL as a Data Query Language is of importance. It is therefore exclusively about searching for and selecting information from databases via SQL statements.\n",
    "\n",
    "The following operators have been considered within this module:\n",
    "* ``SELECT``: Selection of specified or all (``*``) columns\n",
    "* ``DISTINCT``: Display/select an object or feature combination of an object only once, so no duplicates\n",
    "* ``FROM``: Call of the table to retrieve the data from\n",
    "* ``AS``: Assignment of short names for attributes or tables\n",
    "* ``WHERE``: Filter the data based on given statement\n",
    "* ``AND``, ``OR``, ``NOT``, ``IN``, and ``BETWEEN``: Operators can complement filtering and combine expressions\n",
    "* ``ORDER BY``: Sort the results by an attribute/feature/column\n",
    "* ``GROUP BY``: Aggregate the data across the specififed column(s) and with the usage of an aggregation function, e.g.:\n",
    "    * ``sum()``: sum of values\n",
    "    * ``count()``: number of values\n",
    "    * ``min()``: minimum value\n",
    "    * ``max()``: maximum value\n",
    "    * ``avg()``: average/mean of values\n",
    "* ``HAVING``: Filtering in or restricting a ``GROUP BY`` statement\n",
    "* ``LIMIT``: Restriction to limited number of results\n",
    "* ``JOIN`` with ``ON``: Combining different tables based on one or more attributes\n",
    "\n",
    "<div style=\"width:image width px; font-size:75%; text-align:right;\">\n",
    "    <img src=\"img/joins.png\" width=\"800\" style=\"padding-bottom:0.2em;\" />\n",
    "</div>\n",
    "\n",
    "The predefined sequence of SQL commands is:\n",
    "1. ``SELECT``\n",
    "2. ``FROM``\n",
    "3. ``JOIN``\n",
    "3. ``WHERE``\n",
    "4. ``GROUP BY``\n",
    "5. ``HAVING``\n",
    "6. ``ORDER BY``\n",
    "7. ``LIMIT``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Python<a class=\"anchor\" id=\"python\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding Python syntax, all basic operators (including ``+``, ``=`` etc.) and how they work are required. Furthermore, the term \"variable\" should be clear and how it can be assigned. Likewise, very basic functions like ``print()`` or ``len()`` should be understood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collections/containers\n",
    "Python implements four types of data collections for general purposes which are called containers. These are:\n",
    "* **List** is a collection which is *ordered* and *changeable*. Allows *duplicate members*.\n",
    "* **Tuple** is a collection which is *ordered* and *unchangeable*. Allows *duplicate members*.\n",
    "* **Set** is a collection which is *unordered*, *changeable* and *unindexed*. *No duplicate* members.\n",
    "* **Dictionary** is a collection which is *unordered*, *changeable* and *indexed*. *No duplicate* members.\n",
    "\n",
    "The property that elements of a collection can respectively cannot be changed is called mutable respectively immutable. **Please have a look at the lecture notes and how elements of containers can be accessed and edited!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lists\n",
    "A list is a sequence of values, so it is ordered. The values of such a list are called items or elements and can be of any type. By the index you can access the items. Indices in Python start with 0. With negative indices the elements can be accessed backwards, so starting at the end of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listA = ['Mother', 'Father', 'Daughter', 'Son']\n",
    "type(listA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuples\n",
    "The striking difference to lists is that tuples are not changeable - in other words *immutable*. However, their elements are also ordered by indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tupleA = ('Mother', 'Father', 'Daughter', 'Son')\n",
    "type(tupleA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tupleA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sets\n",
    "In contrast to tuples and lists, sets do not have an order and the elements do not have indices. However iteration is possible and you can check whether an element exists in the set. The sets are *mutable*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setA = {'Mother', 'Father', 'Daughter', 'Son'}\n",
    "type(setA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionaries\n",
    "Dictionaries consists of key-value-pairs which are unordered but the pairs have indices and can be changed - so dictionaries are *mutable*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictA = {\n",
    "    'mother':   'Mary',\n",
    "    'father':   'Pete',\n",
    "    'daughter': 'Daisy',\n",
    "    'son':      'Chuck'\n",
    "}\n",
    "type(dictA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictA[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictA['mother']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditions and recursion\n",
    "As a normal case, almost all computer programs, even those that are only slightly more complex or useful, contain processes that depend on conditions or include loops. Of course Python also provides such functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "b = 3\n",
    "\n",
    "if a > b:\n",
    "    print('a is greater than b.')\n",
    "elif a < b:\n",
    "    print('b is greater than a.')\n",
    "else:\n",
    "    print('a and b are equal.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "If a series of instructions are used repeatedly or if one would like to define them separately outside the main program flow for the sake of greater convenience, **functions** can be created. In order to define and use own functions, the user first has to specifiy what the function should do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cust_function(param_a, param_b, param_c = 3):\n",
    "    buffer = param_a + param_b\n",
    "    result = buffer % param_c\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_function(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules and packages\n",
    "Modules are one of the levels of abstraction in Python and **technically nothing more than individual Python files** (``*.py``). These files contain variables and functions that belong together in a certain sense and are therefore implemented or defined separately.\n",
    "\n",
    "To create a module, it is therefore only necessary to write the desired code in a Python file. This file can then be integrated into the current project, for example a Jupyter Notebook, by calling ``import`` followed by the module name which is the file name. It is possible to assign a (abbreviated) name to the included module. This is done with the command ``as``.\n",
    "\n",
    "If the complexity and the structure of a project makes it necessary to combine several modules to a superior unit, so-called packages can be created. These are folders that contain module files. The only special feature is the existence of a ``__init__.py`` in this folder, which contains overall definitions for the whole package.\n",
    "\n",
    "A module of a package is also included by the ``import`` command, preceded by the package name and separated from the module name by a period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## NumPy<a class=\"anchor\" id=\"numpy\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy is the basis for many other Python packages, such as pandas, and provides them data structures and functions for numerical calculations (hence the package name).\n",
    "\n",
    "Important features of the NumPy library are:\n",
    "* Powerful N-dimensional array object ``ndarray``\n",
    "* Mathematical and logical operations on arrays\n",
    "* Generators for random numbers\n",
    "* Fourier transform, trigonometric, statistical and algebraic routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1D-array based on list (vector-like)\n",
    "array1D = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "array1D\n",
    "#print(array1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D-array based on list of lists (table/matrix-like)\n",
    "array2D = np.array([[1, 2], [3, 4], [4, 5], [6, 7]])\n",
    "print(array2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D-array based on list of lists of tuples (cube-like)\n",
    "array3D = np.array([[(9.1, 8.2, 7.3), (6.4, 5.5, 4.6), (3.7, 2.8, 1.9)],\n",
    "                    [(0.1, 0.2, 0.3), (0.4, 0.5, 0.6), (0.7, 0.8, 0.9)]])\n",
    "print(array3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholder arrays\n",
    "arrayZeros    = np.zeros((5, 4, 3, 2))            # fill specificed dimensions with zeros\n",
    "arrayOnes     = np.ones((3, 3))                   # fill specificed dimensions with ones\n",
    "arrayConstant = np.full((3, 3, 3), 3.1415)        # fill specificed dimensions with constant value\n",
    "arrayIdentity = np.eye(3)                         # create identity map (2D) with specified size\n",
    "arrayRandom   = np.random.random((2, 2, 2))       # fill specified dimensions with random numbers\n",
    "arrayRandInt  = np.random.randint(0, 100, (3, 3)) # fill specified dimensions with random integers\n",
    "arrayNormal   = np.random.normal(0, 1, (3, 3))    # fill specified dimensions with normally distributed numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get second row, so index is 1, no specification of columns\n",
    "arrayNormal[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first column, use wildcard for rows\n",
    "arrayNormal[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice second and third element in last row\n",
    "arrayNormal[-1, 1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fancy indexing\n",
    "\n",
    "NumPy implements a method to access arrays, which is called *fancy indexing*. In this method, no numerical indexes are passed directly when the indexing is called, but lists of index values or variables that have stored them. Simple and fancy indexing can also be used in combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayRandInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayRandInt[[1, 0, 1]][:, [1, 2, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayRandInt[[1, 0, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## pandas<a class=\"anchor\" id=\"pandas\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important features of the pandas library are:\n",
    "* Multidimensional, tabular data object ``DataFrame`` for efficient data manipulation and integrated indices.\n",
    "* Direct import of various source formats such as CSV, Excel or SQL\n",
    "* Routines for pre-processing of the data including handling of missing data\n",
    "* Aggregation, merging and joins of data records\n",
    "* Integrated visualization options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas introduces two types of data structures/objects: ``Series`` and ``DataFrame``. ``Series`` is a one-dimensional array with indices. ``DataFrame`` is a two-dimensional, so table-like structure. The various columns can store data of different types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series\n",
    "starts = pd.Series([1949, 1963, 1966, 1969, 1974, 1982, 1998, 2005],\n",
    "                   index = ['Adenauer', 'Erhard', 'Kiesinger', 'Brandt',\n",
    "                            'Schmidt', 'Kohl', 'Schroeder', 'Merkel'])\n",
    "starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, 3],\n",
    "                   'B': [4, 5, 6],\n",
    "                   'C': [7, 8, 9]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import routine - in this case from csv - creates DataFrame directly\n",
    "attrition = pd.read_csv('dat/attrition.csv')\n",
    "attrition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas offers special, optimized methods for data access. We have looked at the two methods ``loc`` and ``iloc`` and have illustrated this with ``DataFrame`` examples.  In addition, we have had also looked at Boolean indexing.\n",
    "\n",
    "* For **label-based** selection pandas provides the ``loc`` function. Here, the name of the rows and/or columns have to be specified in order to select the desired output.\n",
    "* The other method is **index-based** selection of rows and/or columns. This is done with the ``iloc`` function where the indices have do be chosen.\n",
    "\n",
    "*Note: A circumstance that may need practice is the use of square brackets for these functions, thus ``loc[...]`` and ``iloc[...]``.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows by label (Attention: the row names/indices of the baseball example are integers, no need to\n",
    "# specify column names)\n",
    "attrition.loc[[2, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns by label (Attention: The wildcard ':' for the rows must be inserted)\n",
    "attrition.loc[:, 'EducationField'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of rows and columns by label\n",
    "attrition.loc[[10, 12, 15], 'BusinessTravel':'Department']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select by index\n",
    "attrition.iloc[[2, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition.iloc[[10, 12, 15], 1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean indexing in combination with label-based selection of columns\n",
    "attrition[attrition['DistanceFromHome'] == 2].loc[:, 'Attrition':'Education']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Data preprocessing<a class=\"anchor\" id=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every machine learning model creation starts with data preprocessing, where the raw data is converted into a format suitable for the later algorithm(s). This is necessary because real-world data is very often incomplete, inconsistent, unclean, or even contains errors. These shortcomings, which can hinder or prevent the creation of an algorithm, can be completely or partially eliminated by data preprocessing. Thus, preprocessing is a fundamental step that makes machine learning possible.\n",
    "Preprocessing is composed of different steps, each with different objectives:\n",
    "\n",
    "* The so-called **imputation** addresses existing missing values and replaces them, if possible, with suitable substitutions.\n",
    "* Categorical variables are converted into a numerical value during **encoding**, which can be processed by the algorithm in contrast to the categories.\n",
    "* To test the performance of the model, the dataset is split into two sets during the **training-test-split**.\n",
    "* Some algorithms require numerical input in a certain scale range, which is produced by **scaling**.\n",
    "\n",
    "Not every step is required for every algorithm or useful in every approach. This must be examined on a case-by-case basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "The replacement of missing values can be approached by different strategies on a case-by-case basis. From Scikit-learn, the ``SimpleImputer`` class is available for this purpose, which includes ``strategy`` as a parameter:\n",
    "* ``mean``: Replace missing values with mean of the affected column, which has to be numeric.\n",
    "* ``median``: Replace missing values with median of the affected column, which has to be numeric.\n",
    "* ``most_frequent``: Replace missing with the most frequent value of the column (smallest value if there is more than one).\n",
    "* ``constant``: Replace missing values with given ``fill_value``, which is also a parameter of the class.\n",
    "\n",
    "However, the ``fillna()`` method from pandas can also be used here and is particularly useful for different procedures within a data set.\n",
    "\n",
    "*For programming details, please refer to the Jupyter Notebook of the lecture.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical encoding\n",
    "Many machine learning algorithms accept only numerical attributes as input, since they cannot map and process textual information, for example. Therefore, in such cases, encoding the categorical variables becomes necessary. However, care must be taken to avoid introducing additional and incorrect information into the data set. For example, it would not be appropriate to represent \"female\" and \"male\" by 1 and 2. This would generally be associated with a scale level and \"male\" would be considered twice as weighty. For other categorical variables, it may make sense to assign a scale or at least an order, such as clothing sizes, S, M, and L.\n",
    "\n",
    "All categorical variables in our example have no such order. Therefore, we concentrate here on the so-called **\"One Hot Encoding\"**, which inserts a separate column for each expression of the attribute and thus only makes binary statements. For further methods like Ordinal Encoding or more complex approaches, please refer to the literature of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_encode = attrition.loc[:, 'Attrition':'BusinessTravel']\n",
    "df_to_encode.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_encode['BusinessTravel'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df_to_encode, drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For further programming details, please refer to the Jupyter Notebook of the lecture and/or the documentary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test-split\n",
    "\n",
    "The models created in the machine learning process are designed to learn relationships and patterns based on existing data so that they can apply and reproduce them on new, previously unknown data, in effect making a prediction. This property of the models to classify new data appropriately is called **generalization**. It follows that testing a model with training data is a serious methodological error. After all, this would only make the model fit the already known data particularly well. This (perhaps even perfect) result is called **overfitting**. To avoid this, a portion of the available data is set aside as a test set prior to the supervised learning procedure. It is important to make a randomized split here, which can be achieved in scikit-learn with the ``train_test_split`` class.\n",
    "\n",
    "*For programming details, please refer to the Jupyter Notebook of the lecture.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "Variables can include values in very different numerical ranges. Restricting this range so that the attributes can be treated on the same basis or weighting is called scaling.\n",
    "\n",
    "In the context of data analysis, **normalization** and **standardization** are particularly relevant as scaling techniques. *Normalization* means that the data is scaled to the closed interval [0, 1]. With *standardization*, the data is transformed to mean value zero and standard deviation one. Now, which technique is used under which circumstances? There is no general answer to this question. However, some hints can be given that can be considered when making a choice. If the data certainly do not follow a normal distribution, then the obvious choice is to use normalization. In particular, this is useful for modeling that does not assume a particular distribution anyway, such as neural networks. Standardization, on the other hand, can be applied if the data follow a normal distribution, but this is not necessarily the case. The important difference is that standardization, unlike normalization, does not limit the scale of results to a closed interval. This means that the characteristic of outliers remains.\n",
    "\n",
    "For normalization, scikit-learn offers the class ``MinMaxScaler()``, whose default parameters are already set to the interval [0, 1]. However, this interval can also be adjusted to individual intervals such as [-1, 1]. For standardization, the class ``StandardScaler()`` transforms the data to mean value zero and variance one.\n",
    "\n",
    "Important: We first divided the data into a training and a test set and then scaled the data, namely only on the training data. This avoids the so-called **data leakage**. Data leakage means that information that does not belong to the training data set is included in the training. This would be the case if we had first performed the scaling and then the split, since the distribution information of the test set would then have been taken into account during the scaling. This must be ruled out under all circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_scale = attrition.loc[:, 'DistanceFromHome':'Education']\n",
    "df_to_scale.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm_scaler = MinMaxScaler((-1, 1))\n",
    "mm_scaler.fit_transform(df_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "st_scaler = StandardScaler()\n",
    "st_scaler.fit_transform(df_to_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For further programming details, please refer to the Jupyter Notebook of the lecture and/or the documentary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Machine Learning<a class=\"anchor\" id=\"ml\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review the basic concepts and principles of Machine Learning, the PDF document from the lecture \"Introduction Machine Learning\" is extremely relevant. From this, make the following points clear once again:\n",
    "* What does supervised and unsupervised learning mean?\n",
    "* What is accuracy and how is it calculated?\n",
    "* How do you read a confusion matrix?\n",
    "* What do the terms \"false positive\" and \"false negative\" mean?\n",
    "* In addition, review the basic structure of creating machine learning algorithms with Scikit-learn, as in the following two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "\n",
    "accuracy_score(clf.predict(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also possible to calculate accuracy\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "clf = clf.fit(X, y)\n",
    "\n",
    "clf.predict([[1, 1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
